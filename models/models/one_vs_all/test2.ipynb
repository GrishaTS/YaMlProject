{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проверка работоспособности.\n",
    "\n",
    "Я написал 10 моделей one vs all для каждого класса.\n",
    "\n",
    "Сейчас мы попробуем из всех них составить единый файл с ответам и закинуть на kaggle."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "импорт необходимых библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mi\\Desktop\\ML\\.conda\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "sys.path.append('../../../')\n",
    "\n",
    "from core.datasets import open_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulticlassClassificationMetrics:\n",
    "    def __init__(self, y_true, y_pred):\n",
    "        self.y_true = np.array(y_true)\n",
    "        self.y_pred = np.array(y_pred)\n",
    "        self.matrix_error = self.get_matrix_error()\n",
    "\n",
    "    def get_matrix_error(self):\n",
    "        self.class_types = np.unique([self.y_true, self.y_pred])\n",
    "        TP, FN, FP, TN = [], [], [], []\n",
    "        for class_type in self.class_types:\n",
    "            TP.append(np.sum(np.logical_and(self.y_true == self.y_pred, self.y_true == class_type)))\n",
    "            FN.append(np.sum(np.logical_and(self.y_true == class_type, self.y_pred != class_type)))\n",
    "            FP.append(np.sum(np.logical_and(self.y_pred == class_type, self.y_true != class_type)))\n",
    "            TN.append(np.sum(np.logical_and(self.y_pred != class_type, self.y_true != class_type)))\n",
    "        return pd.DataFrame({\n",
    "            'class_type': self.class_types,\n",
    "            'TP': TP, 'TN': TN, 'FP': FP, 'FN': FN,\n",
    "        }).set_index('class_type')\n",
    "\n",
    "    def accuracy(self):\n",
    "        return np.sum(self.y_true == self.y_pred) / self.y_true.shape[0]\n",
    "\n",
    "    def precision(self, averaging='macro'):\n",
    "        if averaging == 'macro':\n",
    "            precisions = self.matrix_error['TP'] / (self.matrix_error['TP'] + self.matrix_error['FP'])\n",
    "            return np.mean(precisions.replace(np.nan, 0))\n",
    "        elif averaging == 'micro':\n",
    "            mean_val = self.matrix_error.mean()\n",
    "            return mean_val['TP'] / (mean_val['TP'] + mean_val['FP'])\n",
    "\n",
    "    def recall(self, averaging='macro'):\n",
    "        if averaging == 'macro':\n",
    "            recalls = self.matrix_error['TP'] / (self.matrix_error['TP'] + self.matrix_error['FN'])\n",
    "            return np.mean(recalls.replace(np.nan, 0))\n",
    "        elif averaging == 'micro':\n",
    "            mean_val = self.matrix_error.mean()\n",
    "            return mean_val['TP'] / (mean_val['TP'] + mean_val['FN'])\n",
    "\n",
    "    def f1_score(self, averaging='macro'):\n",
    "        if averaging == 'macro':\n",
    "            reverse_r = (self.matrix_error['TP'] + self.matrix_error['FN']) / self.matrix_error['TP']\n",
    "            reverse_p = (self.matrix_error['TP'] + self.matrix_error['FP']) / self.matrix_error['TP']\n",
    "            f1_scores = 2 / (reverse_r + reverse_p)\n",
    "            return np.mean(f1_scores.replace(np.nan, 0))\n",
    "        elif averaging == 'micro':\n",
    "            p = self.precision(averaging)\n",
    "            r = self.recall(averaging)\n",
    "            return 2 * p * r / (p + r)\n",
    "\n",
    "    def metrics(self, averaging='macro'):\n",
    "        return {\n",
    "            'accuracy': self.accuracy(),\n",
    "            'precision': self.precision(averaging),\n",
    "            'recall': self.recall(averaging),\n",
    "            'f1_score': self.f1_score(averaging),\n",
    "        }\n",
    "\n",
    "    def __str__(self):\n",
    "        headers = {\n",
    "            'selector': '*',\n",
    "            'props': 'background-color: darkgreen; color: white; font-size: 12pt;',\n",
    "        }\n",
    "        che_super_mega_puper_visualization_of_cell = { \n",
    "            'selector': 'td:hover',\n",
    "            'props': 'background-color: green; color: white;',\n",
    "        }\n",
    "        display(\n",
    "            self.matrix_error\n",
    "            .style\n",
    "            .set_table_styles([\n",
    "                headers,\n",
    "                che_super_mega_puper_visualization_of_cell,\n",
    "            ])\n",
    "            .set_properties(**{'background-color': 'lightgreen',\n",
    "                           'color': 'black', 'font-size': '12pt'})\n",
    "        )\n",
    "        return ''\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            f'MulticlassClassificationMetrics(class_types={self.class_types})'\n",
    "            .replace(\"'\", '').replace(',', ';'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачиваем тестовый датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = open_f('repaired_data_train', back=3)\n",
    "shuffle = np.random.permutation(test_ds['labels'].shape[0])\n",
    "test_ds_x = test_ds['images'][shuffle][:10000] / 255\n",
    "test_ds_y = test_ds['labels'][shuffle][:10000]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Рассмотрим все avg acc чекпоинты "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']\n",
    "models = []\n",
    "for i in nums:\n",
    "    models.append(tf.keras.models.load_model(f'../../checkpoints/model_{i}_avg_categorical_accuracy.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101/313 [========>.....................] - ETA: 49s"
     ]
    }
   ],
   "source": [
    "ans = None\n",
    "for i in models:\n",
    "    pred = i.predict(test_ds_x).ravel()\n",
    "    if type(ans) is type(None):\n",
    "        ans = pred\n",
    "    else:\n",
    "        ans = np.column_stack((ans, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ans = np.argmax(ans, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = MulticlassClassificationMetrics(test_ds_y, full_ans)\n",
    "print(metrics)\n",
    "metrics.metrics()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сперва рассмотрим все acc чекпоинты "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']\n",
    "models = []\n",
    "for i in nums:\n",
    "    models.append(tf.keras.models.load_model(f'../../checkpoints/model_{i}_categorical_accuracy.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = None\n",
    "for i in models:\n",
    "    pred = i.predict(test_ds_x).ravel()\n",
    "    if type(ans) is type(None):\n",
    "        ans = pred\n",
    "    else:\n",
    "        ans = np.column_stack((ans, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ans = np.argmax(ans, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = MulticlassClassificationMetrics(test_ds_y, full_ans)\n",
    "print(metrics)\n",
    "metrics.metrics()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Рассмотрим все avg loss чекпоинты "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']\n",
    "models = []\n",
    "for i in nums:\n",
    "    models.append(tf.keras.models.load_model(f'../../checkpoints/model_{i}_avg_loss.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = None\n",
    "for i in models:\n",
    "    pred = i.predict(test_ds_x).ravel()\n",
    "    if type(ans) is type(None):\n",
    "        ans = pred\n",
    "    else:\n",
    "        ans = np.column_stack((ans, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ans = np.argmax(ans, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = MulticlassClassificationMetrics(test_ds_y, full_ans)\n",
    "print(metrics)\n",
    "metrics.metrics()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Рассмотрим все loss чекпоинты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']\n",
    "models = []\n",
    "for i in nums:\n",
    "    models.append(tf.keras.models.load_model(f'../../checkpoints/model_{i}_loss.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = None\n",
    "for i in models:\n",
    "    pred = i.predict(test_ds_x).ravel()\n",
    "    if type(ans) is type(None):\n",
    "        ans = pred\n",
    "    else:\n",
    "        ans = np.column_stack((ans, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ans = np.argmax(ans, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = MulticlassClassificationMetrics(test_ds_y, full_ans)\n",
    "print(metrics)\n",
    "metrics.metrics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
