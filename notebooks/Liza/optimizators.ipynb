{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Оптимизаторы**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Теория"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Доступные оптимизаторы в tensorflow.keras:\n",
    "- SGD\n",
    "- RMSProp\n",
    "- Adam\n",
    "- AdamW\n",
    "- Adadelta \n",
    "- Adagrad\n",
    "- Adamax\n",
    "- Adafactor\n",
    "- Nadam\n",
    "- Ftrl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим каждый из оптимизаторов отдельно"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. SGD\n",
    "- СУТЬ: обновление весовых коэффициентов нейронной сети с использованием единственного примера i обучающей выборки на каждом шаге.\n",
    "- ПРЕИМУЩЕСТВА:\n",
    "    - Не производит лишних вычислений (в отличие от классического градиентного спуска)\n",
    "- НЕДОСТАТКИ:\n",
    "    - Может быть медленным на большом количестве данных обучения\n",
    "    - Застревает в локальных оптимумах\n",
    "    - Сильно колеблется во время обучения\n",
    "    - Сложно подобрать оптимальные параметры: слишком маленькие будут замедлять обучение, слишком большие будут препятствовать сходимости\n",
    "    \n",
    "Чтобы эти минусы исправить, необходимо понижать скорость обучения модели, но это явно не эффективно (проигрывает другим оптимизаторам)\n",
    "\n",
    "**ВЫВОД**: скорее всего, на больших данных он сработает плохо, но возможно стоит проверить"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее будут описаны оптимизаторы, друг из друга вытекающие"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  Adagrad (адаптивный градиент)\n",
    "- СУТЬ: адаптация SGD: алгоритм уменьшает обновления для элементов, которые и так часто обновляются, и увеличивает обновления для параметров, которые почти не обновлялись.\n",
    "- ПРЕИМУЩЕСТВА:\n",
    "    - исключает необходимость изменения скорости обучения вручную (как это было в SGD)\n",
    "- НЕДОСТАТКИ:\n",
    "    - Накопление суммы квадратов градиентов, отчего уменьшается коэффициент обучения, и в итоге процесс обучения останавливается\n",
    "\n",
    "**Вывод**: неэффективный оптимизатор, дальнейшие его улучшения покажут себя лучше"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. RMSProp\n",
    "- СУТЬ: расширение AdaGrad, где адаптируется размер шага индивидуально для каждого веса, чтобы модель не привыкала к определенным параметрам. Нормы градиентов не просто складываются, а усредняются в скользящем режиме.\n",
    "- ПРЕИМУЩЕСТВА:\n",
    "    - Эффективнее AdaGrad\n",
    "    - Тоже не требует ручной настройки скорости обучения\n",
    "- НЕДОСТАТКИ:\n",
    "    - Новых недостатков относительно AdaGrad не возникает"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод**: стоит проверить на практике"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Adadelta\n",
    "- СУТЬ: расширение AdaGrad, в котором вместо суммы градиентов сохраняется усредненный квадрат истории градиента, чтобы избежать замедления обучения\n",
    "- ПРЕИМУЩЕСТВА:\n",
    "    - Эффективнее AdaGrad\n",
    "    - Тоже не требует ручной настройки скорости обучения\n",
    "- НЕДОСТАТКИ:\n",
    "    - Новых недостатков относительно AdaGrad не возникает"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод**: стоит проверить на практике"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Adam\n",
    "- СУТЬ: Adam = Momentum(метод моментов для ускорения процесса обучения) + RMSprop.\n",
    "- ПРЕИМУЩЕСТВА:\n",
    "    - Включает в себя все достоинства вышеперечисленных оптимизаторов\n",
    "    - Часто используется на практике\n",
    "- НЕДОСТАТКИ:\n",
    "    - Может требоваться большой объём памяти, так как Adam нужно много различных параметров\n",
    "    - Чтобы восстановить модель из чекпоинта, необходимо сохранять не только веса модели, но и накопленные параметры Adam, чтобы он не считал их заново\n",
    "    \n",
    "**Вывод**: скорее всего, этот оптимизатор будет показывать себя лучше остальных. Стоит проверить на практике."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Adamax\n",
    "- СУТЬ: вариант оптимизации по Adam, но без ограничений по норме\n",
    "\n",
    "Благодаря способности регулировать скорость обучения на основе характеристик данных, он подходит для изучения изменяющегося во времени процесса, например, речевых данных с динамически изменяющимися шумовыми условиями. \n",
    "\n",
    "**Вывод**: скорее всего, этот вариант нам не подойдет"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. AdamW\n",
    "- СУТЬ: добавление l2-регуляризации к adam\n",
    "\n",
    "**Вывод**: Этот оптимизатор работает не лучше adam. В данной ситуации регуляризация работает хуже, чем для SGD. Тем более, показательно, что этот оптимизатор почти не используется на практике."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Adafactor\n",
    "- СУТЬ: то же самое, что adam, но затрачивает меньше памяти, потому что сохраняет только частичную информацию о предыдущих градиентах\n",
    "\n",
    "**Вывод**: стоит проверить на практике: затрата меньшего количества памяти звучит хорошо"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Nadam\n",
    "- СУТЬ: развитие оптимизатора Adam – добавление момента Нестерова при вычислении градиентов.\n",
    "\n",
    "**Вывод**: стоит проверить на практике, потому что очевидных минусов у этого оптимизатора нет"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Ftrl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разработан в Google для прогнозирования рейтинга кликов в начале 2010-х годов. Он больше всего подходит для неглубоких моделей с большими и разреженными пространственными элементами.\n",
    "\n",
    "**Вывод**: этот оптимизатор не будет показывать себя хорошо на больших данных, поэтому он нам вряд ли подходит"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вывод: \n",
    "\n",
    "Были выбраны следующие оптимизаторы для проверки на практике: SGD, RMSProp, AdaDelta, Adam, Adafactor, Nadam."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2. Рассмотрим параметры выбранных оптимизаторов:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. SGD\n",
    "Самые важные параметры:\n",
    "- Learning Rate\n",
    "- Momentum\n",
    "- Nesterov"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. RMSProp\n",
    "Самые важные параметры:\n",
    "- Learning Rate\n",
    "- RHO\n",
    "- Momentum"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. AdaDelta\n",
    "Самые важные параметры:\n",
    "- Learning Rate\n",
    "- RHO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Adam\n",
    "Самые важные параметры:\n",
    "- Learning Rate\n",
    "- beta_1\n",
    "- beta_2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. AdaFactor\n",
    "Самые важные параметры:\n",
    "- Learning Rate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Nadam\n",
    "Самые важные параметры:\n",
    "- Learning Rate\n",
    "- beta_1\n",
    "- beta_2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TODO - объяснение каждого параметра и его важности"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Rate играет огромную роль в любом оптимизаторе. Его эффективнее писать не через floating point, a через LearningRateSchedule.\n",
    "\n",
    "\\# TODO объяснение всего\n",
    "\n",
    "Его проще реализовать через класс с методами __init__ и __call__\n",
    "\n",
    "Пример представлен ниже"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "  def __init__(self, initial_learning_rate):\n",
    "    self.initial_learning_rate = initial_learning_rate\n",
    "\n",
    "  def __call__(self, step):\n",
    "     return self.initial_learning_rate / (step + 1)\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=MyLRSchedule(0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Напишем модели, которые покажут каждый оптимизатор на практике"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
